---
layout: post
title: K8s 실전 활용을 위한 10단계 Ch.08
subtitle: ''
categories: devops
tags: k8s
comments: false
---

## 디플로이먼트

디플로이먼트의 역할에 대해 이해해보자.

1장에서 언급했듯이 디플로이먼트는 백엔드의 워크로드에 적합한 **컨트롤러**이다.

디플로이먼트의 주 역할은 파드의 개수를 관리하는 것이다.

k8s 클러스터에서 파드는 서버의 역할을 담당하며 처리 능력을 높이고 싶다면 **파드의 개수를 늘리면** 된다.

물론 불필요하게 많은 파드를 구동시키면 CPU와 메모리가 낭비될 수 있으므로 적절히 조절해야한다.

이처럼 파드의 개수를 관리하는 것은 시스템의 처리 능력과 서비스를 중단하지 않는 가용성, 그리고 비용 측면에서 매우 중요하다.

디플로이먼트는 요청한 개수만큼 파드를 기동하며 장애 등의 이유로 파드의 개수가 줄어들면 새롭게 파드를 만들어 기동한다.

그리고 애플리케이션의 버전을 업그레이드할 때 새로운 버전의 파드로 조금씩 바꾸는 기능(롤링 업데이트) 기능도 제공한다.

그런데 로드밸런서에 해당하는 기능은 디플로이먼트에 포함되지 않는데, 이는 쿠버네티스의 **서비스**라는 오브젝트가 담당하고 있다.

디플로이먼트는 혼자 동작하지 않고 **레플리카**와 함게 동작한다.

이 관계는 아래의 그림과 같다.

![B80Z9](https://user-images.githubusercontent.com/43809168/103437408-d5ac1100-4c6a-11eb-910a-823fd93438a2.png)

레플리카셋은 디플로이먼트의 매니페스트에 적힌 레플리카의 값을 기준으로 파드의 개수를 조절하며 사용자가 직접 조작하는 경우는 거으이 없고 주로 상태를 참조한다.

## 디플로이먼트 생성과 삭제

디플로이먼트를 만들고 지워보자.

디플로이먼트를 만드는 명렁어는 `kubectl apply -f {YAML_파일명}`이고 지울 때는 `kubectl delete -f {YAML_파일명}`이다.

아래는 디플로이먼트 매니페스트이다.

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web # 매치 라벨은 디플로이먼트와 파드를 엮을 라벨 명이다.
  template:
    metadata:
      labels:
        app: web # 디플로이먼트에 정의된 라벨을 적어준다.
    spec: # spec은 template 안에 적어야한다.
      containers:
        - name: nginx
          image: 'nginx:latest' # 도커허브로부터 이미지를 가져온다.
```

```
 ✘  ~/TestDatas  kubectl apply -f deployment.yml
deployment.apps/web-deploy created
 ✘  ~/TestDatas  kubectl get deploy
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
web-deploy   2/3     3            2           10s
 ~/TestDatas  kubectl get rs
NAME                    DESIRED   CURRENT   READY   AGE
web-deploy-86cd4d65b9   3         3         3       83s
 ~/TestDatas  kubectl get po -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP           NODE       NOMINATED NODE   READINESS GATES
web-deploy-86cd4d65b9-gvg5l   1/1     Running   0          87s   172.17.0.3   minikube   <none>           <none>
web-deploy-86cd4d65b9-p7hn8   1/1     Running   0          87s   172.17.0.5   minikube   <none>           <none>
web-deploy-86cd4d65b9-sb956   1/1     Running   0          87s   172.17.0.4   minikube   <none>           <none>
```

위와 같은 로그를 확인할 수 있다.

미니쿠베의 경우 단일 노드를 사용하므로 3개의 파드가 같은 노드에 배치된다.

만약 노드가 여러개라면 k8s 클러스터는 여러 노드에 파드를 분산 배치시킨다.

## 스케일 기능

스케일 기능은 레플리카의 값을 변경하는 기능을 의미한다.

레플리카에 명시된 파드의 개수를 조절하여 처리 능력을 높이거나 낮추는 기능이다.

여기서는 수동으로 변경하는 방법을 사용하지만 CPU 사용량과 연동해서 동적으로 레플리카의 값을 조절하는 오토스케일 기능도 존재한다.

이미 배포된 디플로이먼트의 레플리카 값을 변경하여 처리 능력을 올려보자.

```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-deploy
spec:
  replicas: 10 # 10으로 증가시켜보았다.
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
        - name: nginx
          image: 'nginx:latest'
```

```
 ~/TestDatas  kubectl apply -f deployment.yml
deployment.apps/web-deploy configured

 ~/TestDatas  kubectl get po
NAME                          READY   STATUS    RESTARTS   AGE
web-deploy-86cd4d65b9-49tf4   1/1     Running   0          33s
web-deploy-86cd4d65b9-7lj77   1/1     Running   0          33s
web-deploy-86cd4d65b9-bdjd6   1/1     Running   0          33s
web-deploy-86cd4d65b9-dsjdh   1/1     Running   0          33s
web-deploy-86cd4d65b9-gqvw6   1/1     Running   0          33s
web-deploy-86cd4d65b9-gvg5l   1/1     Running   0          5m40s
web-deploy-86cd4d65b9-p7hn8   1/1     Running   0          5m40s
web-deploy-86cd4d65b9-r25ts   1/1     Running   0          33s
web-deploy-86cd4d65b9-sb956   1/1     Running   0          5m40s
web-deploy-86cd4d65b9-xlxxr   1/1     Running   0          33s
```

디플로이먼트를 통해 늘릴 수 있는 것은 파드의 개수 뿐이다.

파드의 개수를 늘리는 중에 k8s 클러스터의 자원(CPU, 메모리)이 부족해서 노드를 추가하여 자원이 생길 때 까지 파드 생성이 미뤄진다.

메모리가 부족한 상황에서 까지 가상 메모리를 사용해 메인 메모리를 페이징하면서 까지 파드를 늘려주지는 않는다고 한다.

메인 메모리로 파드를 배치할 수 없으면 파드는 생성되지 않는다.

따라서 파드의 개수를 늘리기 전에 클러스터의 가용 자원을 확인해서 노드 증설을 검토할 필요가 있다.

### 실제 서비스 환경에서

이미 배포된 서비스의 경우 CI에서 버저닝이 따지는데 해당 버전의 이미지로 레플리카 수를 조절해볼 수 있을 것 같다.

가령 트래픽이 증가할 수 있는 상황에서는 따로 빌드-배포를 하지 않고 로컬에서 레플리카 수만 조절하는 것도 가능하지 않을까 싶다.

그러면 순간적으로 트래픽이 몰리는 시간대를 대비해서 파드 수를 늘려서 처리력을 높일 수 있을 것 같다.

## 롤아웃 기능

롤아웃은 쿠버네티스에서 **컨테이너의 업데이트**를 의미한다.

롤아웃을 할 때는 사용자의 요청을 처리할 수 있도록 정해진 개수만큼 업데이트를 진행할 수 있다.

쿠버네티스 롤아웃은 별다른 설정을 하지 않으면 기본은 **RollingUpdateStrategy**라는 것을 사용한다.

```
~/TestDatas  kubectl describe deployment web-deploy
Name:                   web-deploy
Namespace:              default
CreationTimestamp:      Fri, 01 Jan 2021 22:51:09 +0900
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
                        kubectl.kubernetes.io/last-applied-configuration:
                          {"apiVersion":"apps/v1","kind":"Deployment","metadata":{"annotations":{},"name":"web-deploy","namespace":"default"},"spec":{"replicas":10,...
Selector:               app=web
Replicas:               10 desired | 10 updated | 10 total | 10 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge // 이 부분 주목
Pod Template:
  Labels:  app=web
  Containers:
   nginx:
    Image:        nginx:latest
    Port:         <none>
    Host Port:    <none>
    Environment:  <none>
    Mounts:       <none>
  Volumes:        <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   web-deploy-86cd4d65b9 (10/10 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  18m   deployment-controller  Scaled up replica set web-deploy-86cd4d65b9 to 3
  Normal  ScalingReplicaSet  13m   deployment-controller  Scaled up replica set web-deploy-86cd4d65b9 to 10
```

위 디플로이먼트 상태에서 `RollingUpdateStrategy: 25% max unavailable, 25% max surge`를 확인할 수 있다.

최대 25%의 파드를 정지할 수 있고, 최대 25%만큼 초과할 수 있음을 의미한다.

가령 레필라카 값이 10이면, 최소 파드수는 `10-10X0.25=7.5`라는 계산식이 나오는데 소수점을 올려 8이 된다.

초과 파드수는 12.5이므로 13이 된다.

정리하면 최소 8개의 파드를 유지하면서 최대 13개의 파드가 되도록 롤아웃이 점진적으로 진행된다.

롤아웃에 의해 종료되는 파드에는 종료 요청 시그널 SIGTERM이 전송된다.

만약 컨테이너에서 애플리케이션이 이 시그널을 받아 종료 처리를 하지 않으면 30초 후에 SIGKILL에 의해 강제 종료 된다.

종료 요청을 받은 컨테이너는 더 이상 클라이언트로부터 새로운 요청을 전달받지 않게 된다.

그리고 종료 시그널을 받은 후 메인 프로세스를 종료시킨다.

## 롤백 기능

이전 버전의 애플리케이션으로 되돌리는 **Rollback** 기능에 대해 알아보자.

k8s에서는 아주 간단한 명령어로 롤백을 수행할 수 있다.

`kubectl rollout undo deployment web-deploy` 한 줄이면 된다.

## 파드의 IP 주소가 변경되는 경우와 아닌 경우

파드의 IP 주소는 특정 이벤트에 의해 변할 수도 있고 변하지 않을 수도 있다.

기본적으로는 가동시에 할당되며 종료 시에 회수되어 다른 파드에 할당된다.

롤아웃이나 롤백, 스케일에 의해 파드가 종료되고 새롭게 만들어지는 경우라면 새로운 IP 주소가 할당된다고 볼 수 있다.

한편 파드가 컨테이너의 프로세스가 이상 종료를 한 경우 재시작하여 RESTARTS 값이 증가한 경우라면 파드의 IP 주소는 바뀌지 않는다.

## 자동 복구

파드 내의 컨테이너가 모종의 이유로 죽으면, 기본적으로는 파드가 컨테이너를 재구동 시켜준다.

즉, 파드는 컨테이너 수준의 장애에 대한 복구를 시도한다.

이에 반해 디플로이먼트의 자동 복구 기능은 파드 단위로 복구한다.

지금부터 하드웨어에 장애가 났음을 가정하고 k8s 클러스터에 다음과 같은 상황을 연출해보려고 한다.

1. 파드가 단독으로 기동하는 매니페스트를 작성

2. 디플로이먼트의 매니페스트를 작성

3. 두 매니페스트를 배포

4. 단독 파드가 배치된 노드를 정지시켜 자동 복구 동작을 확인

5. 다시 노드를 기동하여 파드의 상태를 확인

이를 위해 Google Cloud의 GKE를 사용하였다.

```
gcloud config set project amugeonago # 프로젝트 설정
gcloud config set compute/zone asia-northeast3-a # 리전 설정
gcloud container clusters create test --num-nodes=2 // 클러스터 생성 노드 수는 2개
```

<img width="1006" alt="스크린샷 2021-01-02 오전 12 00 29" src="https://user-images.githubusercontent.com/43809168/103440959-8deab100-4c8d-11eb-8810-62913db84941.png">

다음과 같은 에러가 발생하면 아래의 명령어로 API 서비스를 사용해야한다

[참고 글](https://stackoverflow.com/questions/64537546/error-gcloud-container-clusters-create-responseerror-code-400-message-faile)

```
gcloud services enable container.googleapis.com

gcloud config set compute/zone asia-northeast3-a
```

이렇게 되면 현재 쿠버네티스 클러스터는 GCP의 클러스터와 로컬 미니쿠베 클러스터 2개가 되는데 이를 스위칭하는 명령어는 다음과 같다.

```
kubectl config use-context CONTEXT_NAME # 현재 클러스터를 설정하는 명령어

kubectl config get-contexts # 현재 클러스터들을 보는 명령어
```

클러스터 설정이 완료되면 다음 명령어로 해당 클러스터와 구글 계정의 인증을 완료해주자

```
gcloud container clusters get-credentials test
```

그리고 테스트를 위해 아래의 서비스들을 배포해주자

```yaml
# 파드 단독으로 구동하는 매니페스트
---
apiVersion: v1
kind: Pod
metadata:
  name: test1
spec:
  containers:
    - command:
        - sh
        - '-c'
        - sleep 3600; exit 0
      image: 'busybox:1'
      name: busybox
  restartPolicy: Always
```

```yaml
# 디플로이먼트로 배포되는 매니페스트
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test2
spec:
  replicas: 4
  selector:
    matchLabels:
      app: test2
  template:
    metadata:
      labels:
        app: test2
    spec:
      containers:
        - command:
            - sh
            - '-c'
            - sleep 3600; exit 0
          image: 'busybox:1'
          name: busybox
```

배포 이후 다음의 명령어로 단독으로 기동한 파드가 어느 노드에 배포되어있는지 확인해주자

```
kubectl get node
kubectl describe node {노드 이름}

or

kubectl get pod -o wide
```

필자의 경우 `gke-test-default-pool-4c7aa6cb-6lh5`라는 이름의 노드에 test1 파드가 배치되었으므로 이 노드를 중지해보고 어떤 일이 일어나는지 확인해보자.

## 작성 중

## Reference

![k8s](https://user-images.githubusercontent.com/43809168/101032998-6684c380-35bd-11eb-8ba7-a784fd46b37a.png)

### 15단계로 배우는 도커와 쿠버네티스 - 타카라 마호 저서
